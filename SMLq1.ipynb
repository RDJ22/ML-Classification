{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "#### 1. Derive the Splitting Criterion for a Decision Tree\n",
    "\n",
    "##### a. Gini Impurity Definition\n",
    "\n",
    "The Gini Impurity is a metric used to evaluate the impurity of a node in a decision tree. For a node with $k$ classes $C_1, C_2, \\dots, C_k$, the Gini Impurity is defined as:\n",
    "$$\n",
    "\\text{Gini}(p) = 1 - \\sum_{i=1}^k p_i^2\n",
    "$$\n",
    "where:\n",
    "- $p_i$ is the proportion of observations in class $C_i$ at the node.\n",
    "\n",
    "The Gini Impurity ranges between $0$ (perfect purity) and $1 - \\frac{1}{k}$ (maximum impurity, when all classes are equally represented).\n",
    "\n",
    "##### b. Gini Impurity for Splitting\n",
    "\n",
    "When splitting a node into two child nodes $N_L$ and $N_R$, the total Gini Impurity after the split is a weighted average:\n",
    "$$\n",
    "\\text{Gini}_{\\text{split}} = \\frac{n_L}{n} \\text{Gini}(N_L) + \\frac{n_R}{n} \\text{Gini}(N_R)\n",
    "$$\n",
    "where:\n",
    "- $n$: Total number of observations at the parent node.\n",
    "- $n_L, n_R$: Number of observations in the left and right child nodes.\n",
    "- $\\text{Gini}(N_L)$: Gini Impurity of the left child node.\n",
    "- $\\text{Gini}(N_R)$: Gini Impurity of the right child node.\n",
    "\n",
    "##### c. Splitting Criterion\n",
    "\n",
    "To select the best split, we calculate $\\text{Gini}_{\\text{split}}$ for each possible feature and threshold, and choose the split that minimizes $\\text{Gini}_{\\text{split}}$.\n",
    "\n",
    "\n",
    "#### 2. How Random Forest Improves Over a Single Decision Tree\n",
    "\n",
    "##### a. Bootstrapping\n",
    "\n",
    "Random Forests use **bootstrapping** to create multiple Decision Trees. In bootstrapping:\n",
    "- Random subsets (with replacement) of the training data are used to train each tree.\n",
    "- This introduces diversity among trees and reduces the risk of overfitting, as each tree sees a slightly different dataset.\n",
    "\n",
    "##### b. Feature Bagging\n",
    "\n",
    "Random Forests also use **feature bagging** (random feature selection):\n",
    "- At each split, a random subset of features is considered for splitting rather than using all features.\n",
    "- This ensures that individual trees do not over-rely on dominant features, enhancing diversity.\n",
    "\n",
    "##### c. Advantages of Random Forest\n",
    "\n",
    "- **Reduced Variance**: Combining predictions from multiple trees (via averaging for regression or majority voting for classification) reduces variability, leading to more stable predictions.\n",
    "- **Reduced Overfitting**: Decision Trees can overfit to training data, especially when deep. By averaging over multiple trees, Random Forests mitigate this risk.\n",
    "- **Better Generalization**: The combination of bootstrapping and feature bagging helps Random Forests generalize better to unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comparison Summary\n",
    "\n",
    "| **Aspect**               | **Decision Tree**                            | **Random Forest**                     |\n",
    "|--------------------------|----------------------------------------------|---------------------------------------|\n",
    "| **Model Type**           | Single Tree                                  | Ensemble of Trees                    |\n",
    "| **Splitting Criterion**  | Gini Impurity or Entropy                     | Same as Decision Tree                |\n",
    "| **Training Data**        | Full dataset                                 | Bootstrapped subsets                 |\n",
    "| **Features for Splits**  | All features available                       | Random subset of features            |\n",
    "| **Overfitting**          | High risk for deep trees                     | Reduced due to ensembling            |\n",
    "| **Variance**             | High (model is sensitive to data changes)    | Lower (due to averaging predictions) |\n",
    "| **Performance**          | Moderate                                    | Generally better due to ensemble     |\n",
    "\n",
    "##### Random Forest achieves higher accuracy and robustness by combining multiple trees than a single Decision Tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<brb>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<brb>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
